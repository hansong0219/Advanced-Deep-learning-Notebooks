{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+03eQcf2ZT0yCvTdGF0kn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hansong0219/Advanced-DeepLearning-Study/blob/master/improved_GAN/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgURjbwjdbHK"
      },
      "source": [
        "# WGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEDKe3EwZMbx"
      },
      "source": [
        "from tensorflow.keras.layers import Activation, Dense, Input\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyAITo6EiCEC"
      },
      "source": [
        "# 생성기와 판별기 등 함수 구성 \n",
        "생성기와 판별기의 함수는 DCGAN의 구성을 그대로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUDGEiHiC-C"
      },
      "source": [
        "def build_generator(inputs,\n",
        "              image_size,\n",
        "              activation='sigmoid'):\n",
        "\n",
        "    image_resize = image_size // 4\n",
        "    kernel_size = 5\n",
        "    layer_filters = [128, 64, 32, 1]\n",
        "    \n",
        "    x = inputs\n",
        "    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n",
        "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
        "\n",
        "    for filters in layer_filters:\n",
        "        if filters > layer_filters[-2]:\n",
        "            strides = 2\n",
        "        else:\n",
        "            strides = 1\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2DTranspose(filters=filters,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=strides,\n",
        "                            padding='same')(x)\n",
        "\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "\n",
        "    return Model(inputs, x, name='generator')\n",
        "\n",
        "\n",
        "def build_discriminator(inputs,\n",
        "                  activation='sigmoid'):\n",
        "\n",
        "    kernel_size = 5\n",
        "    layer_filters = [32, 64, 128, 256]\n",
        "\n",
        "    x = inputs\n",
        "    for filters in layer_filters:\n",
        "        # first 3 convolution layers use strides = 2\n",
        "        # last one uses strides = 1\n",
        "        if filters == layer_filters[-1]:\n",
        "            strides = 1\n",
        "        else:\n",
        "            strides = 2\n",
        "        x = LeakyReLU(alpha=0.2)(x)\n",
        "        x = Conv2D(filters=filters,\n",
        "                   kernel_size=kernel_size,\n",
        "                   strides=strides,\n",
        "                   padding='same')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    # default output is probability that the image is real\n",
        "    outputs = Dense(1)(x)\n",
        "    if activation is not None:\n",
        "        print(activation)\n",
        "        outputs = Activation(activation)(outputs)\n",
        "\n",
        "    return Model(inputs, outputs, name='discriminator')\n",
        "\n",
        "\n",
        "def plot_images(generator,\n",
        "                noise_input,\n",
        "                noise_label=None,\n",
        "                noise_codes=None,\n",
        "                show=False,\n",
        "                step=0,\n",
        "                model_name=\"gan\"):\n",
        "  \n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "    filename = os.path.join(model_name, \"%05d.png\" % step)\n",
        "    rows = int(math.sqrt(noise_input.shape[0]))\n",
        "    if noise_label is not None:\n",
        "        noise_input = [noise_input, noise_label]\n",
        "        if noise_codes is not None:\n",
        "            noise_input += noise_codes\n",
        "\n",
        "    images = generator.predict(noise_input)\n",
        "    plt.figure(figsize=(2.2, 2.2))\n",
        "    num_images = images.shape[0]\n",
        "    image_size = images.shape[1]\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i + 1)\n",
        "        image = np.reshape(images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.savefig(filename)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close('all')\n",
        "\n",
        "\n",
        "def test_generator(generator):\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "    plot_images(generator,\n",
        "                noise_input=noise_input,\n",
        "                show=True,\n",
        "                model_name=\"test_outputs\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y87zX4F7koYU"
      },
      "source": [
        "# WGAN 구현 \n",
        "\n",
        "GAN 과 유사하게 생성기와 판별기를 교대로 훈련 시키지만 구현에 있어서 WGAN 의 가장 큰 특징은 Wasserstein Loss 를 사용함과 동시에 생성기를 1회 훈련시키기전에 판별기를 n 회 훈련 시킨다. \n",
        "\n",
        "이는 생성기와 판별기를 동일한 횟수로 훈련시키는 GAN 과는 다른 점이다. 판별기를 훈련시킨다는 것은 판별기의 매개변수를 학습한다는 것을 뜻한다. \n",
        "\n",
        "또, EMD의 제약 조건을 만족 시키기 위해 위해 립시츠 제약에 대한 변수 또한 필요하다. 립시츠 조건이란 두 점사이의 거리를 일정 비율 이상 증가시키지 않는다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBa0cTmFNS6K"
      },
      "source": [
        "def wasserstein_loss(y_label, y_pred):\n",
        "  return - K.mean(y_label*y_pred)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b38xV9Mhksoy"
      },
      "source": [
        "#MNIST 데이터 세트 로딩\n",
        "(x_train,_),(_,_) = mnist.load_data()\n",
        "\n",
        "# 데이터 형상 변환 및 정규화\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32')/255\n",
        "  \n",
        "model_name = \"wgan_mnist\"\n",
        "\n",
        "#네트워크 매개 변수 지정 \n",
        "latent_size = 100\n",
        "#WGAN 에서 추가된 매개변수\n",
        "n_critic = 5\n",
        "clip_value = 0.01\n",
        "\n",
        "batch_size = 64\n",
        "lr = 5e-5\n",
        "train_steps = 40000\n",
        "input_shape = (image_size, image_size, 1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc70e6giMnX1",
        "outputId": "17efa4c7-f634-46d1-b066-11bfcbefd697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "#판별기 모델 구성\n",
        "inputs = Input(shape=input_shape, name = 'discriminator_input')\n",
        "\n",
        "#WGAN 은 선형 activation 을 사용한다. \n",
        "discriminator = build_discriminator(inputs, activation='linear')\n",
        "optimizer = RMSprop(lr=lr)\n",
        "\n",
        "#WGAN 판별기는 Wasserstein loss 를 사용한다.\n",
        "discriminator.compile(loss=wasserstein_loss,optimizer=optimizer,metrics=[\"accuraty\"])\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 14, 14, 32)        832       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 64)          51264     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 4097      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,080,577\n",
            "Trainable params: 1,080,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msp_hSBRNdVG",
        "outputId": "85f33fdc-6890-4744-8a28-f8da9736d887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "#생성기 모델 구성\n",
        "input_shape = (latent_size,)\n",
        "inputs = Input(shape=input_shape, name = 'z_input')\n",
        "generator = build_generator(inputs, image_size)\n",
        "generator.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_input (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,301,505\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adGFv60ENv_u",
        "outputId": "0111f940-4ee5-4751-e09a-aa8f1176fd1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#적대적 모델 구성\n",
        "\n",
        "#적대적 네트워크를 훈련하는 동안 판별기의 가중치는 고정\n",
        "discriminator.trainable = False\n",
        "adversarial = Model(inputs, discriminator(generator(inputs)),name = model_name)\n",
        "\n",
        "adversarial.compile(loss=wasserstein_loss, optimizer=optimizer,metrics=['accuracy'])\n",
        "adversarial.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"wgan_mnist\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_input (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "generator (Functional)       (None, 28, 28, 1)         1301505   \n",
            "_________________________________________________________________\n",
            "discriminator (Functional)   (None, 1)                 1080577   \n",
            "=================================================================\n",
            "Total params: 2,382,082\n",
            "Trainable params: 1,300,801\n",
            "Non-trainable params: 1,081,281\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bUizp30Okl7"
      },
      "source": [
        "models = (generator, discriminator, adversarial)\n",
        "params = (batch_size, latent_size, n_critic, clip_value, train_steps, model_name)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xTOTWpO6qB"
      },
      "source": [
        "# WGAN 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nr5xVOHO6Ok"
      },
      "source": [
        "def train(models, x_train, params):\n",
        "  #판별기와 적대적 네트워크를 배치단위로 교대로 훈련 \n",
        "  \"\"\"\n",
        "  먼저 판별기가 제대로 레이블이 붙은 진짜와 가짜이미지를 사용해 n_critic 번 훈련된다.\n",
        "  판별기 가중치는 립시츠 조건에 따라 범위가 제한된다.\n",
        "  다음으로 생성기가 가짜 이미지를 진짜인 것 처럼 적대적네트워크를 통해 훈련된다.\n",
        "  \"\"\"\n",
        "\n",
        "  # 입력 인수 \n",
        "  \"\"\"\n",
        "  models(list) : Generator, DIscriminator, Adversarial 모델\n",
        "  x_train (tensor) : 이미지 훈련\n",
        "  params (list) : 네트워크 매개변수\n",
        "  \"\"\"\n",
        "\n",
        "  #GAN 모델\n",
        "  generator, discriminator, adversarial = models\n",
        "\n",
        "  #네트워크 매개변수\n",
        "  (batch_size, latent_size, n_critic, clip_value, train_steps, model_name) = params\n",
        "\n",
        "  #이미지 생성 단계\n",
        "  save_interval = 500\n",
        "\n",
        "  #훈련하는 동안의 생성기 출력을 확인하기 위한 노이즈 벡터\n",
        "  noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
        "  #훈련 데이터 세트의 수 \n",
        "  train_size = x_train.sahpe[0]\n",
        "  # 실제 데이터의 레이블\n",
        "  real_labels = np.ones((batch_size, 1))\n",
        "\n",
        "  for i in range (train_steps):\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    #판별기를 n_critic 회 만큼 훈련을 우선 시킨다.\n",
        "    for i in range(n_critic):\n",
        "      # 배치별 판별기 훈련 \n",
        "      # 실제 이미지와 label = 1, 가짜이미지 label = -1 로 구성된 1배치 \n",
        "      # 데이터 셋에서  실제 이미지를 임의로 선정한다.\n",
        "      rand_indices = np.random.randint(0, train_size, size=batch_size)\n",
        "      real_images = x_train[rand_indices]\n",
        "      \n",
        "      # 생성기를 이용하여 가짜 이미지를 생성한다\n",
        "      noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "      fake_images = generater.predict(noise)\n",
        "      \n",
        "      #판별기 네트워크의 훈련\n",
        "      \"\"\"\n",
        "      - 진짜 데이터 레이블 = 1 , 가짜 데이터 레이블 = -1 로 구성한다.\n",
        "      - 진짜와 가짜 이미지를 결합해 하나의 배치를 만드는 대신, 처음에는 진짜 데이터로 구성된 하나의 배치로 훈련한 다음 가짜 이미지로 구성된 하나의 배치로 훈련 하는 형태이다.\n",
        "      (위와 같이 훈련함으로써 진짜와 가짜 데이터 레이블의 부호가 반대고 범위제한으로 인해 가중치의 크기가 작아서 경사가 소실되는 것을 방지한다.)\n",
        "      \"\"\"\n",
        "      real_loss, real_acc = discriminator.train_on_batch(real_images, real_labels)\n",
        "      fake_loss, fake_acc = discriminator.train_on_batch(fake_images,-real_labels)\n",
        "\n",
        "      #평균 손실과 정확도를 누적 \n",
        "      loss += 0.5*(real_loss+fake_loss)\n",
        "      acc += 0.5*(real_acc+fake_acc)\n",
        "\n",
        "      #립시츠 제약 사항을 만족하기 위한 판별치 가중 범위 제한\n",
        "      for layer in discriminator.layers:\n",
        "        weights = layer.get_weights()\n",
        "        weights = [np.clip(weight, -clip_value, clip_value) for weight in weights]\n",
        "\n",
        "    # n_critic 회 반복 훈련하는 동안 평균 손실과 정확도 \n",
        "    loss /= n_critic\n",
        "    acc /= n_critic\n",
        "    log = \"%d: [discriminator loss: %f, acc: %f]\" %(i, loss, acc) \n",
        "\n",
        "    #1 배치 동안 적대적 네트워크의 훈련\n",
        "    noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "\n",
        "    #적대적 네트워크 훈련 \n",
        "    loss, acc = adversarial.train_on_batch(noise, real_labels)\n",
        "\n",
        "    log = \"%s [adversarial loss : %f, acc: %f]\" %(log, loss, acc)\n",
        "    print(log)\n",
        "    \n",
        "    if (i+1) % save_interval == 0:\n",
        "      if (i+1) == train_steps:\n",
        "        show = True\n",
        "      else:\n",
        "        show = False\n",
        "\n",
        "      plot_images(generator, noise_input= noise_input, show=show, step=(i+1),model_name = model_name)\n",
        "\n",
        "  generator.save(model_name + '.h5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfWlBq4sO5F_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}